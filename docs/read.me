The LLM Flows offers a method for extracting LLM prompts from the final code and allows a chain of multiple prompts to filter or validate  with code the results of a prompt execution.



## How to use  llmFlows in your code:

```javascript
/* Check the llmExampletest for complete code. The main code schema is as bellow */

let flowsFactory = require('llmFlows').createFlowsFactory;
let flows = flowsFactory({...});
flows.registerFlow("generateTitles", {...}
    let ret = await flows.callAsync("generateTitles", "An article about the best way to cook a steak");

```

## Remote & Local APIs recommendations

The flows factory, as well as the code from the flows, does not impose any APIs or communication protocol with the expectation of this.retun(value) syntax.
However, we recommend to use the following APIs:

### setDefaultValues()   

### setCostsLevel(number_between_0_and_10)

### setIntelligenceLevel(number_between_0_and_10)

### setCreativityLevel(number_between_0_and_10)

### request(prompt, numberOfOptions, max_tokens)

### requestAs(personalityName, prompt, numberOfOptions, max_tokens)

### brainstorm(prompt, numberOfOptions, max_tokens_per_option)

### brainstormAs(personalityName, prompt, numberOfOptions, max_tokens_per_option)

### proofread(personalityName, prompt)

### definePersonality(personalityName, personalityDescription)




